"""
Connecteur unifiÃ© pour les diffÃ©rents LLMs.
Supporte Mistral (par dÃ©faut) et DeepSeek.
"""

import os
import time
import json
import threading
import signal
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List, Union
from datetime import datetime, timedelta
import httpx

import numpy as np
from mistralai import Mistral

from dotenv import load_dotenv

load_dotenv()

from utils.logger import setup_logger, log_llm_interaction, log_llm_complete_exchange
from config import default_config
from core.global_rate_limiter import global_rate_limiter

# CrÃ©er un verrou global unique pour tous les appels LLM
_llm_execution_lock = threading.Lock()

class TimeoutError(Exception):
    """Exception levÃ©e en cas de timeout LLM."""
    pass

def timeout_handler(signum, frame):
    """Handler pour le timeout."""
    raise TimeoutError("Timeout LLM atteint")


class LLMConnector(ABC):
    """
    Classe abstraite pour les connecteurs LLM.
    """
    
    @abstractmethod
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        agent_context: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> str:
        """
        GÃ©nÃ¨re une rÃ©ponse Ã  partir d'un prompt.
        
        Args:
            prompt: Prompt utilisateur
            system_prompt: Prompt systÃ¨me (optionnel)
            **kwargs: ParamÃ¨tres supplÃ©mentaires pour l'API (temperature, max_tokens, etc.)
            
        Returns:
            str: RÃ©ponse gÃ©nÃ©rÃ©e
        """
        pass

    @abstractmethod
    def generate_with_messages(
        self,
        messages: List[Dict[str, str]],
        agent_context: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> str:
        """GÃ©nÃ¨re avec historique de messages."""
        pass

    @abstractmethod
    def generate_json(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        schema: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        GÃ©nÃ¨re une rÃ©ponse JSON structurÃ©e.
        
        Args:
            prompt: Prompt utilisateur
            system_prompt: Prompt systÃ¨me
            schema: SchÃ©ma JSON attendu (pour validation)
            **kwargs: ParamÃ¨tres supplÃ©mentaires pour l'API (temperature, max_tokens, etc.)
            
        Returns:
            Dict: RÃ©ponse JSON parsÃ©e
        """
        pass


class MistralConnector(LLMConnector):
    """
    Connecteur pour l'API Mistral.
    """
    
    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        """
        Initialise le connecteur Mistral.
        
        Args:
            api_key: ClÃ© API Mistral (utilise l'env var si non fournie)
            model: ModÃ¨le Ã  utiliser (utilise le dÃ©faut si non fourni)
        """
        self.api_key = api_key or os.getenv("MISTRAL_API_KEY")
        if not self.api_key:
            raise ValueError("ClÃ© API Mistral non trouvÃ©e. DÃ©finissez MISTRAL_API_KEY dans .env")
        
        self.client = Mistral(api_key=self.api_key)
        self.model = model or default_config['llm']['default_model']
        self.logger = setup_logger("MistralConnector")

        # Configuration des modÃ¨les Mistral
        self.model_configs = {
            model_info['name']: model_info 
            for model_info in default_config['llm']['models']['mistral']['models']
        }

    def _enforce_rate_limit(self):
        """Applique le rate limiting global pour tous les appels API."""
        global_rate_limiter.enforce_rate_limit(f"MistralConnector({self.model})")

    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        agent_context: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> str:
        """
        GÃ©nÃ¨re une rÃ©ponse avec Mistral.
        GÃ¨re la prioritÃ© des paramÃ¨tres : appel > config > dÃ©faut.
        """
        
        with _llm_execution_lock:
        
            start_time = time.time()
            
            # Fusion des paramÃ¨tres de configuration
            # Commence avec les paramÃ¨tres LLM globaux de la config
            params = default_config['llm'].copy()
            # Retire les clÃ©s de configuration qui ne sont pas des paramÃ¨tres d'API
            params.pop('models', None)
            params.pop('model_preferences', None)
            params.pop('default_model', None)

            # 2. On fusionne avec les paramÃ¨tres spÃ©cifiques au modÃ¨le, qui ont la prioritÃ©.
            model_specific_params = self.model_configs.get(self.model, {}).copy()
            model_specific_params.pop('name', None) # Retirer la clÃ© 'name'
            params.update(model_specific_params)

            # 3. On fusionne avec les paramÃ¨tres de l'appel (kwargs), qui ont la prioritÃ© la plus Ã©levÃ©e.
            params.update(kwargs)




            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            try:
                # Retry logic
                max_retries = default_config['general']['max_retries']
                retry_delay = default_config['general']['retry_delay']
                
                for attempt in range(max_retries):
                    if attempt > 0:
                        self.logger.info(f"ðŸ”„ Tentative {attempt + 1}/{max_retries} aprÃ¨s Ã©chec")
                    try:
                        # Appliquer le rate limiting global avant la requÃªte
                        self._enforce_rate_limit()
                        
                        # Configuration du timeout
                        timeout_seconds = int(default_config['general']['llm_timeout'])
                        
                        # DÃ©finir le handler de timeout
                        old_handler = signal.signal(signal.SIGALRM, timeout_handler)
                        signal.alarm(timeout_seconds)
                        
                        try:
                            # On passe les paramÃ¨tres fusionnÃ©s directement Ã  l'API
                            response = self.client.chat.complete(
                                model=self.model,
                                messages=messages,
                                **params
                            )
                        finally:
                            # Toujours annuler le timeout et restaurer le handler
                            signal.alarm(0)
                            signal.signal(signal.SIGALRM, old_handler)
                        
                        result = response.choices[0].message.content
                        duration = time.time() - start_time
                        tokens_used = response.usage.total_tokens if hasattr(response, 'usage') else None
                        
                        # LOG COMPLET ET EXHAUSTIF
                        log_llm_complete_exchange(
                            agent_name=agent_context.get('agent_name', 'Unknown') if agent_context else 'Unknown',
                            model=self.model,
                            messages=messages,
                            response=result,
                            parameters=params,
                            tokens_used=tokens_used,
                            duration=duration,
                            context=agent_context or {}
                        )
                        
                        # Log l'interaction (ancien systÃ¨me pour compatibilitÃ©)
                        log_llm_interaction(
                            self.logger,
                            prompt=prompt,
                            response=result,
                            model=self.model,
                            tokens_used=tokens_used,
                            duration=duration
                        )
                        

                        return result
                        
                    except TimeoutError as e:
                        duration = time.time() - start_time
                        error_msg = f"ðŸš« TIMEOUT LLM aprÃ¨s {timeout_seconds}s (durÃ©e rÃ©elle: {duration:.1f}s) - ModÃ¨le: {self.model}"
                        self.logger.error(error_msg)
                        
                        if attempt < max_retries - 1:
                            self.logger.warning(f"Nouvelle tentative {attempt + 2}/{max_retries} aprÃ¨s timeout...")
                            time.sleep(retry_delay)
                        else:
                            raise Exception(f"TIMEOUT LLM DÃ‰FINITIF: {error_msg}")
                        
                    except Exception as e:
                        if "timeout" in str(e).lower() or "time" in str(e).lower():
                            duration = time.time() - start_time
                            error_msg = f"ðŸš« TIMEOUT LLM dÃ©tectÃ© aprÃ¨s {duration:.1f}s - ModÃ¨le: {self.model} - Erreur: {str(e)}"
                            self.logger.error(error_msg)
                            
                            if attempt < max_retries - 1:
                                self.logger.warning(f"Nouvelle tentative {attempt + 2}/{max_retries} aprÃ¨s timeout...")
                                time.sleep(retry_delay)
                            else:
                                raise Exception(f"TIMEOUT LLM DÃ‰FINITIF: {error_msg}")
                        else:
                            if attempt < max_retries - 1:
                                self.logger.warning(f"Tentative {attempt + 1} Ã©chouÃ©e: {str(e)}")
                                time.sleep(retry_delay)
                            else:
                                raise
                            
            except Exception as e:
                self.logger.error(f"Erreur lors de la gÃ©nÃ©ration Mistral: {str(e)}")
                raise

    def generate_with_messages(
        self,
        messages: List[Dict[str, str]],
        agent_context: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> str:
        """GÃ©nÃ©ration avec historique complet."""

        with _llm_execution_lock:

            start_time = time.time()
            
            params = self.model_configs.get(self.model, {}).copy()
            params.pop('name', None)
            params.update(kwargs)
            
            try:
                # Retry logic
                max_retries = default_config['general']['max_retries']
                retry_delay = default_config['general']['retry_delay']
                
                for attempt in range(max_retries):
                    if attempt > 0:
                        self.logger.info(f"ðŸ”„ Tentative {attempt + 1}/{max_retries} aprÃ¨s Ã©chec")
                    try:
                        # Appliquer le rate limiting global avant la requÃªte
                        self._enforce_rate_limit()
                        
                        # Configuration du timeout
                        timeout_seconds = int(default_config['general']['llm_timeout'])
                        
                        # DÃ©finir le handler de timeout
                        old_handler = signal.signal(signal.SIGALRM, timeout_handler)
                        signal.alarm(timeout_seconds)
                        
                        try:
                            response = self.client.chat.complete(
                                model=self.model,
                                messages=messages,
                                **params
                            )
                        finally:
                            # Toujours annuler le timeout et restaurer le handler
                            signal.alarm(0)
                            signal.signal(signal.SIGALRM, old_handler)
                        
                        result = response.choices[0].message.content
                        duration = time.time() - start_time
                        tokens_used = response.usage.total_tokens if hasattr(response, 'usage') else None
                        
                        # LOG COMPLET ET EXHAUSTIF
                        log_llm_complete_exchange(
                            agent_name=agent_context.get('agent_name', 'Unknown') if agent_context else 'Unknown',
                            model=self.model,
                            messages=messages,
                            response=result,
                            parameters=params,
                            tokens_used=tokens_used,
                            duration=duration,
                            context=agent_context or {}
                        )
                        
                        return result
                        
                    except TimeoutError as e:
                        duration = time.time() - start_time
                        error_msg = f"ðŸš« TIMEOUT LLM aprÃ¨s {timeout_seconds}s (durÃ©e rÃ©elle: {duration:.1f}s) - ModÃ¨le: {self.model}"
                        self.logger.error(error_msg)
                        
                        if attempt < max_retries - 1:
                            self.logger.warning(f"Nouvelle tentative {attempt + 2}/{max_retries} aprÃ¨s timeout...")
                            time.sleep(retry_delay)
                        else:
                            raise Exception(f"TIMEOUT LLM DÃ‰FINITIF: {error_msg}")
                            
                    except Exception as e:
                        if "timeout" in str(e).lower() or "time" in str(e).lower():
                            duration = time.time() - start_time
                            error_msg = f"ðŸš« TIMEOUT LLM dÃ©tectÃ© aprÃ¨s {duration:.1f}s - ModÃ¨le: {self.model} - Erreur: {str(e)}"
                            self.logger.error(error_msg)
                            
                            if attempt < max_retries - 1:
                                self.logger.warning(f"Nouvelle tentative {attempt + 2}/{max_retries} aprÃ¨s timeout...")
                                time.sleep(retry_delay)
                            else:
                                raise Exception(f"TIMEOUT LLM DÃ‰FINITIF: {error_msg}")
                        else:
                            if attempt < max_retries - 1:
                                self.logger.warning(f"Tentative {attempt + 1} Ã©chouÃ©e: {str(e)}")
                                time.sleep(retry_delay)
                            else:
                                raise
                            
            except Exception as e:
                self.logger.error(f"Erreur lors de la gÃ©nÃ©ration Mistral avec messages: {str(e)}")
                raise
    

    def generate_json(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        schema: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        GÃ©nÃ¨re une rÃ©ponse JSON avec Mistral.
        """
        # Enrichir le prompt pour demander du JSON
        json_prompt = f"{prompt}\n\nRÃ©ponds uniquement avec un JSON valide, sans texte avant ou aprÃ¨s."
        
        if schema:
            json_prompt += f"\n\nLe JSON doit suivre ce schÃ©ma:\n{json.dumps(schema, indent=2)}"
        
        # Ajouter une instruction systÃ¨me pour le JSON
        if system_prompt:
            system_prompt += "\nTu dois toujours rÃ©pondre avec du JSON valide uniquement."
        else:
            system_prompt = "Tu es un assistant qui rÃ©pond toujours avec du JSON valide uniquement, sans texte supplÃ©mentaire."
        
        # DÃ©finir une tempÃ©rature basse par dÃ©faut pour le JSON, sauf si surchargÃ©e.
        kwargs.setdefault('temperature', 0.3)
        
        response = self.generate(
            prompt=json_prompt,
            system_prompt=system_prompt,
            **kwargs
        )
        
        # Utiliser le parser JSON centralisÃ© robuste
        from core.json_parser import get_json_parser
        
        parser = get_json_parser(f"LLMConnector.{self.__class__.__name__}")
        result = parser.parse_universal(response, return_type='dict')
        
        if not result:
            self.logger.error(f"Ã‰CHEC TOTAL parsing JSON avec toutes les stratÃ©gies")
            self.logger.debug(f"RÃ©ponse brute: {response}")
            return {"error": "Could not parse JSON with any strategy", "raw_response": response}
        
        # Valider contre le schÃ©ma si fourni
        if schema:
            self._validate_json_schema(result, schema)
        
        #time.sleep(1)
        return result
    
    def _validate_json_schema(self, data: Dict[str, Any], schema: Dict[str, Any]) -> None:
        """
        Validation basique du schÃ©ma JSON.
        """
        # ImplÃ©mentation simple - vÃ©rifier les clÃ©s requises
        if 'required' in schema:
            for key in schema['required']:
                if key not in data:
                    raise ValueError(f"ClÃ© requise manquante: {key}")
    
    def _try_repair_json(self, response: str) -> Dict[str, Any]:
        """
        DEPRECATED: Utilise maintenant le parser centralisÃ© robuste.
        """
        from core.json_parser import get_json_parser
        
        parser = get_json_parser(f"LLMConnector.RepairJSON")
        result = parser.parse_universal(response, return_type='dict')
        
        if not result:
            return {"error": "Could not parse JSON with any strategy", "raw_response": response}
        
        return result




class MistralEmbedConnector:
    """Connecteur pour l'API Mistral Embed."""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("MISTRAL_API_KEY")
        if not self.api_key:
            raise ValueError("ClÃ© API Mistral non trouvÃ©e. DÃ©finissez MISTRAL_API_KEY dans .env")
        
        self.client = Mistral(api_key=self.api_key)
        self.logger = setup_logger("MistralEmbedConnector")
        self.embedding_dimension = 1024  # Dimension fixe de Mistral Embed

    def _enforce_rate_limit(self):
        """Applique le rate limiting global pour tous les appels API."""
        global_rate_limiter.enforce_rate_limit("MistralEmbedConnector")

    def embed_texts(self, texts: List[str]) -> np.ndarray:
        """GÃ©nÃ¨re des embeddings pour une liste de textes."""
        try:
            # Retry logic
            max_retries = default_config['general']['max_retries']
            retry_delay = default_config['general']['retry_delay']
            
            for attempt in range(max_retries):
                if attempt > 0:
                    self.logger.info(f"ðŸ”„ Tentative {attempt + 1}/{max_retries} aprÃ¨s Ã©chec")
                try:
                    # Appliquer le rate limiting global avant la requÃªte
                    self._enforce_rate_limit()
                                
                    response = self.client.embeddings.create(
                        model="mistral-embed",
                        inputs=texts
                    )
                    
                    # Convertir en numpy array
                    embeddings = np.array([embedding.embedding for embedding in response.data])
                    
                    # Normaliser pour cosinus similarity
                    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
                    normalized_embeddings = embeddings / norms
                    
                    return normalized_embeddings
                    
                except Exception as e:
                    if attempt < max_retries - 1:
                        self.logger.warning(f"Tentative {attempt + 1} Ã©chouÃ©e pour embeddings: {str(e)}")
                        time.sleep(retry_delay)
                    else:
                        raise
                        
        except Exception as e:
            self.logger.error(f"Erreur Mistral Embed: {str(e)}")
            raise




class DeepSeekConnector(LLMConnector):
    """
    Connecteur pour l'API DeepSeek.
    """
    
    def __init__(self, api_key: Optional[str] = None, model: str = "deepseek-chat"):
        """
        Initialise le connecteur DeepSeek.
        
        Args:
            api_key: ClÃ© API DeepSeek
            model: ModÃ¨le Ã  utiliser
        """
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        if not self.api_key:
            raise ValueError("ClÃ© API DeepSeek non trouvÃ©e. DÃ©finissez DEEPSEEK_API_KEY dans .env")
        
        self.base_url = "https://api.deepseek.com/v1"
        self.model = model
        self.logger = setup_logger("DeepSeekConnector")
        
        # Client HTTP
        self.client = httpx.Client(
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            timeout=float(default_config['general']['llm_timeout'])
        )

    def _enforce_rate_limit(self):
        """Applique le rate limiting global pour tous les appels API."""
        global_rate_limiter.enforce_rate_limit(f"DeepSeekConnector({self.model})")
    
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        agent_context: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> str:
        """
        GÃ©nÃ¨re une rÃ©ponse avec DeepSeek.
        """
        
        with _llm_execution_lock:
            start_time = time.time()
            
            # Logique de fusion des paramÃ¨tres
            params = {
                'temperature': 0.7,
            }
            params.update(kwargs)
            
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            payload = {
                "model": self.model,
                "messages": messages,
                "stream": False,
                **params
            }
            
            try:
                # Retry logic
                max_retries = default_config['general']['max_retries']
                retry_delay = default_config['general']['retry_delay']
                
                for attempt in range(max_retries):
                    if attempt > 0:
                        self.logger.info(f"ðŸ”„ Tentative {attempt + 1}/{max_retries} aprÃ¨s Ã©chec")
                    try:
                        # Appliquer le rate limiting global avant la requÃªte
                        self._enforce_rate_limit()
                        
                        response = self.client.post(
                            f"{self.base_url}/chat/completions",
                            json=payload
                        )
                        response.raise_for_status()
                        
                        data = response.json()
                        result = data['choices'][0]['message']['content']
                        duration = time.time() - start_time
                        
                        # LOG COMPLET ET EXHAUSTIF
                        log_llm_complete_exchange(
                            agent_name=agent_context.get('agent_name', 'Unknown') if agent_context else 'Unknown',
                            model=self.model,
                            messages=messages,
                            response=result,
                            parameters=params,
                            tokens_used=data.get('usage', {}).get('total_tokens'),
                            duration=duration,
                            context=agent_context or {}
                        )
                        
                        # Log l'interaction (ancien systÃ¨me pour compatibilitÃ©)
                        log_llm_interaction(
                            self.logger,
                            prompt=prompt,
                            response=result,
                            model=self.model,
                            tokens_used=data.get('usage', {}).get('total_tokens'),
                            duration=duration
                        )
                        
                        return result
                        
                    except httpx.TimeoutException as e:
                        duration = time.time() - start_time  
                        timeout_seconds = default_config['general']['llm_timeout']
                        error_msg = f"ðŸš« TIMEOUT DeepSeek aprÃ¨s {timeout_seconds}s (durÃ©e rÃ©elle: {duration:.1f}s) - ModÃ¨le: {self.model}"
                        self.logger.error(error_msg)
                        
                        if attempt < max_retries - 1:
                            self.logger.warning(f"Nouvelle tentative {attempt + 2}/{max_retries} aprÃ¨s timeout DeepSeek...")
                            time.sleep(retry_delay)
                        else:
                            raise Exception(f"TIMEOUT DEEPSEEK DÃ‰FINITIF: {error_msg}")
                            
                    except Exception as e:
                        if "timeout" in str(e).lower() or "time" in str(e).lower():
                            duration = time.time() - start_time
                            timeout_seconds = default_config['general']['llm_timeout']
                            error_msg = f"ðŸš« TIMEOUT DeepSeek dÃ©tectÃ© aprÃ¨s {duration:.1f}s - ModÃ¨le: {self.model} - Erreur: {str(e)}"
                            self.logger.error(error_msg)
                            
                            if attempt < max_retries - 1:
                                self.logger.warning(f"Nouvelle tentative {attempt + 2}/{max_retries} aprÃ¨s timeout DeepSeek...")
                                time.sleep(retry_delay)
                            else:
                                raise Exception(f"TIMEOUT DEEPSEEK DÃ‰FINITIF: {error_msg}")
                        else:
                            if attempt < max_retries - 1:
                                self.logger.warning(f"Tentative {attempt + 1} Ã©chouÃ©e: {str(e)}")
                                time.sleep(retry_delay)
                            else:
                                raise
                            
            except Exception as e:
                self.logger.error(f"Erreur lors de la gÃ©nÃ©ration DeepSeek: {str(e)}")
                raise


    def generate_with_messages(
        self,
        messages: List[Dict[str, str]],
        agent_context: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> str:
        """DeepSeek avec historique."""
        
        with _llm_execution_lock:
            start_time = time.time()
            
            payload = {
                "model": self.model,
                "messages": messages,
                "stream": False,
                **kwargs
            }
            
            try:
                # Retry logic
                max_retries = default_config['general']['max_retries']
                retry_delay = default_config['general']['retry_delay']
                
                for attempt in range(max_retries):
                    if attempt > 0:
                        self.logger.info(f"ðŸ”„ Tentative {attempt + 1}/{max_retries} aprÃ¨s Ã©chec")
                    try:
                        # Appliquer le rate limiting global avant la requÃªte
                        self._enforce_rate_limit()
                        
                        response = self.client.post(
                            f"{self.base_url}/chat/completions",
                            json=payload
                        )
                        response.raise_for_status()
                        
                        data = response.json()
                        result = data['choices'][0]['message']['content']
                        duration = time.time() - start_time
                        
                        # LOG COMPLET ET EXHAUSTIF
                        log_llm_complete_exchange(
                            agent_name=agent_context.get('agent_name', 'Unknown') if agent_context else 'Unknown',
                            model=self.model,
                            messages=messages,
                            response=result,
                            parameters=kwargs,
                            tokens_used=data.get('usage', {}).get('total_tokens'),
                            duration=duration,
                            context=agent_context or {}
                        )
                        
                        return result
                        
                    except httpx.TimeoutException as e:
                        duration = time.time() - start_time  
                        timeout_seconds = default_config['general']['llm_timeout']
                        error_msg = f"ðŸš« TIMEOUT DeepSeek aprÃ¨s {timeout_seconds}s (durÃ©e rÃ©elle: {duration:.1f}s) - ModÃ¨le: {self.model}"
                        self.logger.error(error_msg)
                        
                        if attempt < max_retries - 1:
                            self.logger.warning(f"Nouvelle tentative {attempt + 2}/{max_retries} aprÃ¨s timeout DeepSeek...")
                            time.sleep(retry_delay)
                        else:
                            raise Exception(f"TIMEOUT DEEPSEEK DÃ‰FINITIF: {error_msg}")
                            
                    except Exception as e:
                        if "timeout" in str(e).lower() or "time" in str(e).lower():
                            duration = time.time() - start_time
                            timeout_seconds = default_config['general']['llm_timeout']
                            error_msg = f"ðŸš« TIMEOUT DeepSeek dÃ©tectÃ© aprÃ¨s {duration:.1f}s - ModÃ¨le: {self.model} - Erreur: {str(e)}"
                            self.logger.error(error_msg)
                            
                            if attempt < max_retries - 1:
                                self.logger.warning(f"Nouvelle tentative {attempt + 2}/{max_retries} aprÃ¨s timeout DeepSeek...")
                                time.sleep(retry_delay)
                            else:
                                raise Exception(f"TIMEOUT DEEPSEEK DÃ‰FINITIF: {error_msg}")
                        else:
                            if attempt < max_retries - 1:
                                self.logger.warning(f"Tentative {attempt + 1} Ã©chouÃ©e: {str(e)}")
                                time.sleep(retry_delay)
                            else:
                                raise
                            
            except Exception as e:
                self.logger.error(f"Erreur lors de la gÃ©nÃ©ration DeepSeek avec messages: {str(e)}")
                raise
    



    def generate_json(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        schema: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        GÃ©nÃ¨re une rÃ©ponse JSON avec DeepSeek.
        """
        json_prompt = f"{prompt}\n\nRÃ©ponds uniquement avec un JSON valide."
        
        if schema:
            json_prompt += f"\n\nSchÃ©ma attendu:\n{json.dumps(schema, indent=2)}"
        
        if system_prompt:
            system_prompt += "\nRÃ©ponds toujours en JSON valide."
        else:
            system_prompt = "Tu rÃ©ponds toujours en JSON valide uniquement."
        
        kwargs.setdefault('temperature', 0.3)
        
        response = self.generate(
            prompt=json_prompt,
            system_prompt=system_prompt,
            **kwargs
        )
        
        # Utiliser le parser JSON centralisÃ© robuste
        from core.json_parser import get_json_parser
        
        parser = get_json_parser(f"DeepSeekConnector")
        result = parser.parse_universal(response, return_type='dict')
        
        if not result:
            self.logger.error("Erreur de parsing JSON")
            return {"error": "Invalid JSON", "raw": response}
        
        return result
    
    def __del__(self):
        """Ferme le client HTTP."""
        if hasattr(self, 'client'):
            self.client.close()


class LLMFactory:
    """
    Factory pour crÃ©er des connecteurs LLM avec pattern singleton.
    RÃ©utilise les instances existantes pour Ã©conomiser les ressources.
    """
    
    _connectors = {
        'mistral': MistralConnector,
        'deepseek': DeepSeekConnector,
        'mistral-embed': MistralEmbedConnector
    }
    
    # Registre des instances crÃ©Ã©es (singleton par modÃ¨le)
    _instances = {}
    _lock = threading.Lock()
    
    @classmethod
    def create(
        cls,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        **kwargs
    ) -> LLMConnector:
        """
        CrÃ©e ou rÃ©cupÃ¨re un connecteur LLM (singleton par modÃ¨le).
        
        Args:
            provider: Fournisseur (mistral, deepseek)
            model: ModÃ¨le spÃ©cifique
            **kwargs: Arguments supplÃ©mentaires (ignorÃ©s pour le cache)
            
        Returns:
            LLMConnector: Instance du connecteur (partagÃ©e)
        """
        # DÃ©terminer le provider depuis le modÃ¨le si non fourni
        if not provider and model:
            provider = cls._detect_provider(model)
        
        # Utiliser le provider par dÃ©faut si non fourni
        if not provider:
            default_model = default_config['llm']['default_model']
            provider = cls._detect_provider(default_model)
            model = model or default_model
        
        if provider not in cls._connectors:
            raise ValueError(f"Provider non supportÃ©: {provider}")
        
        # ClÃ© de cache basÃ©e sur le modÃ¨le uniquement
        cache_key = f"{provider}:{model}"
        
        # Thread-safe singleton pattern
        with cls._lock:
            if cache_key not in cls._instances:
                connector_class = cls._connectors[provider]
                # CrÃ©er une nouvelle instance (adapter les paramÃ¨tres selon le connecteur)
                if provider == 'mistral-embed':
                    # MistralEmbedConnector ne prend pas de paramÃ¨tre model
                    cls._instances[cache_key] = connector_class()
                else:
                    # Les autres connecteurs prennent un paramÃ¨tre model
                    cls._instances[cache_key] = connector_class(model=model)
            
            return cls._instances[cache_key]
    
    @classmethod
    def _detect_provider(cls, model_name: str) -> str:
        """
        DÃ©tecte le provider depuis le nom du modÃ¨le.
        """
        if 'mistral' in model_name or 'codestral' in model_name or 'magistral' in model_name:
            return 'mistral'
        elif 'deepseek' in model_name:
            return 'deepseek'
        else:
            # Par dÃ©faut, utiliser Mistral
            return 'mistral'
    
    @classmethod
    def register_connector(cls, name: str, connector_class: type):
        """
        Enregistre un nouveau connecteur.
        
        Args:
            name: Nom du provider
            connector_class: Classe du connecteur
        """
        cls._connectors[name] = connector_class
    
    @classmethod
    def clear_cache(cls):
        """Nettoie le cache des instances (utile pour les tests)."""
        with cls._lock:
            # Fermer les connexions existantes
            for instance in cls._instances.values():
                if hasattr(instance, 'client') and hasattr(instance.client, 'close'):
                    try:
                        instance.client.close()
                    except:
                        pass  # Ignorer les erreurs de fermeture
            cls._instances.clear()
    
    @classmethod 
    def get_instance_count(cls) -> int:
        """Retourne le nombre d'instances en cache (utile pour les tests)."""
        return len(cls._instances)