"""
Classe de base abstraite pour tous les agents de la plateforme.
Version orient√©e outils : les agents utilisent des outils atomiques.
"""

from collections import deque
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List, Callable, Union
from datetime import datetime
import uuid
import threading
import json
import time
from pathlib import Path
import re
import json5

from utils.logger import get_project_logger, LoggerAdapter
from core.llm_connector import LLMFactory
from core.lightweight_llm_service import get_lightweight_llm_service
from config import default_config


class ToolResult:
    """R√©sultat standardis√© d'un outil."""
    
    def __init__(self, status: str, result: Any = None, artifact: Optional[str] = None, error: Optional[str] = None):
        self.status = status  # 'success' ou 'error'
        self.result = result
        self.artifact = artifact  # Chemin vers l'artifact cr√©√©
        self.error = error
        self.timestamp = datetime.now().isoformat()
        
    def to_dict(self) -> Dict[str, Any]:
        return {
            'status': self.status,
            'result': self.result,
            'artifact': self.artifact,
            'error': self.error,
            'timestamp': self.timestamp
        }


class Tool:
    """Classe de base pour un outil."""
    
    def __init__(self, name: str, description: str, parameters: Dict[str, str]):
        self.name = name
        self.description = description
        self.parameters = parameters  # {"param_name": "description"}
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'name': self.name,
            'description': self.description,
            'parameters': self.parameters
        }


class BaseAgent(ABC):
    """
    Classe abstraite d√©finissant l'interface commune pour tous les agents.
    Architecture orient√©e outils.
    """
    
    def __init__(
        self,
        name: str,
        role: str,
        personality: str,
        llm_config: Dict[str, Any],
        project_name: str,
        supervisor: Optional['BaseAgent'] = None,
        rag_engine: Optional[Any] = None
    ):
        """
        Initialise un agent avec syst√®me d'outils.
        """
        self.id = str(uuid.uuid4())
        self.name = name
        self.role = role
        self.personality = personality
        self.llm_config = llm_config
        self.project_name = project_name
        self.supervisor = supervisor
        
        # √âtat interne
        self.state = {
            'status': 'idle',
            'current_task': None,
            'current_task_id': None,
            'completed_tasks': [],
            'created_at': datetime.now().isoformat()
        }
        
        # Logger avec contexte
        self.logger = get_project_logger(project_name, name)
        
        # Historique des interactions
        self.conversation_history: List[Dict[str, Any]] = []
        
        # RAG partag√©
        self.rag_engine = rag_engine
        self.current_milestone_id = None
        
        # M√©moire conversationnelle
        memory_size = default_config['general'].get('conversation_memory_size', 5)
        self.conversation_memory = deque()  # Pas de limite, compression g√®re la taille
        self._memory_lock = threading.Lock()
        
        # Syst√®me d'outils
        self.tools: Dict[str, Callable] = {}
        self.tool_definitions: Dict[str, Tool] = {}
        self._register_common_tools()
        
        # Configuration des outils
        self.tools_config = default_config.get('tools', {})
        
        # Communication
        communication_config = default_config.get('communication', {})
        self.communication_enabled = communication_config.get('enabled', False)
        self.max_exchanges = communication_config.get('max_exchanges_per_task', 5)
        self.current_exchanges = {}
        
        # Guidelines
        self.guidelines = self._load_guidelines()
        
        # CYCLE COGNITIF HYBRIDE - Service l√©ger pour phase d'alignement
        self.lightweight_service = get_lightweight_llm_service(self.project_name)
        
        self.logger.info(f"Agent {name} initialis√© avec architecture orient√©e outils + service l√©ger")
    
    def register_tool(self, tool: Tool, implementation: Callable) -> None:
        """Enregistre un nouvel outil."""
        self.tools[tool.name] = implementation
        self.tool_definitions[tool.name] = tool
        self.logger.debug(f"Outil '{tool.name}' enregistr√©")
    
    def _register_common_tools(self) -> None:
        """Enregistre les outils communs √† tous les agents."""
        
        # search_context
        self.register_tool(
            Tool(
                "search_context",
                "Recherche du contexte pertinent dans le RAG",
                {
                    "query": "Requ√™te de recherche",
                    "top_k": "Nombre de r√©sultats (optionnel, d√©faut: 5)"
                }
            ),
            self._tool_search_context
        )
        
        # send_message_to_agent
        self.register_tool(
            Tool(
                "send_message_to_agent",
                "Envoie un message/question √† un autre agent",
                {
                    "agent_name": "Nom de l'agent destinataire",
                    "message": "Message ou question √† envoyer"
                }
            ),
            self._tool_send_message_to_agent
        )
        
        # share_discovery
        self.register_tool(
            Tool(
                "share_discovery",
                "Partage une d√©couverte importante dans la m√©moire de travail",
                {
                    "discovery": "Description de la d√©couverte",
                    "importance": "Niveau d'importance (low/normal/high/critical)"
                }
            ),
            self._tool_share_discovery
        )
        
        # report_to_supervisor
        self.register_tool(
            Tool(
                "report_to_supervisor",
                "Envoie un rapport au superviseur",
                {
                    "report_type": "Type de rapport (progress/issue/completion)",
                    "content": "Contenu du rapport"
                }
            ),
            self._tool_report_to_supervisor
        )
    
    def _tool_search_context(self, parameters: Dict[str, Any]) -> ToolResult:
        """Impl√©mentation de l'outil search_context."""
        try:
            if not self.rag_engine:
                return ToolResult('error', error="RAG engine non disponible")
            
            query = parameters.get('query', '')
            top_k = int(parameters.get('top_k', 5))
            
            results = self.rag_engine.search(query, top_k=top_k)
            
            # Formater les r√©sultats
            formatted_results = []
            for r in results:
                formatted_results.append({
                    'text': r.get('chunk_text', '')[:200],
                    'source': r.get('source', 'unknown'),
                    'score': r.get('score', 0)
                })
            
            return ToolResult('success', result=formatted_results)
            
        except Exception as e:
            return ToolResult('error', error=str(e))
    
    def _tool_send_message_to_agent(self, parameters: Dict[str, Any]) -> ToolResult:
        """Impl√©mentation de l'outil send_message_to_agent."""
        try:
            if not self.communication_enabled:
                return ToolResult('error', error="Communication inter-agents d√©sactiv√©e")
            
            agent_name = parameters.get('agent_name', '').lower()
            message = parameters.get('message', '')
            
            task_id = self.state.get('current_task_id', 'unknown')
            
            # V√©rifier la limite d'√©changes
            exchanges_count = self.current_exchanges.get(task_id, 0)
            if exchanges_count >= self.max_exchanges:
                return ToolResult('error', error=f"Limite d'√©changes atteinte ({self.max_exchanges})")
            
            # Obtenir l'agent via le superviseur
            if not self.supervisor:
                return ToolResult('error', error="Pas de superviseur pour la communication")
            
            colleague = self.supervisor.get_agent(agent_name)
            if not colleague:
                return ToolResult('error', error=f"Agent {agent_name} non trouv√©")
            
            # Envoyer le message
            response = colleague.answer_colleague(self.name, message)
            
            # Mettre √† jour le compteur
            self.current_exchanges[task_id] = exchanges_count + 1
            
            return ToolResult('success', result={'response': response, 'from': agent_name})
            
        except Exception as e:
            return ToolResult('error', error=str(e))
    
    def _tool_share_discovery(self, parameters: Dict[str, Any]) -> ToolResult:
        """Impl√©mentation de l'outil share_discovery."""
        try:
            discovery = parameters.get('discovery', '')
            importance = parameters.get('importance', 'normal')
            
            if not self.rag_engine:
                return ToolResult('error', error="RAG engine non disponible")
            
            # Valider l'importance
            valid_importance = ['low', 'normal', 'high', 'critical']
            if importance not in valid_importance:
                importance = 'normal'
            
            # Cr√©er le message
            prefix = {
                'critical': 'üö® CRITIQUE',
                'high': '‚ö†Ô∏è IMPORTANT',
                'normal': '‚ÑπÔ∏è Info',
                'low': 'üí° Note'
            }.get(importance, '‚ÑπÔ∏è Info')
            
            message = f"{prefix} - {self.name}: {discovery}"
            
            # Indexer dans la m√©moire de travail
            self.rag_engine.index_to_working_memory(
                message,
                {
                    'type': 'discovery',
                    'agent_name': self.name,
                    'importance': importance,
                    'milestone': self.current_milestone_id or 'unknown'
                }
            )
            
            return ToolResult('success', result={'discovery_shared': True})
            
        except Exception as e:
            return ToolResult('error', error=str(e))
    
    def _tool_report_to_supervisor(self, parameters: Dict[str, Any]) -> ToolResult:
        """Impl√©mentation de l'outil report_to_supervisor."""
        try:
            if not self.supervisor:
                return ToolResult('error', error="Pas de superviseur assign√©")
            
            report_type = parameters.get('report_type', 'progress')
            content = parameters.get('content', {})
            
            # Construire le rapport
            report = {
                'type': report_type,
                'agent': self.name,
                'timestamp': datetime.now().isoformat(),
                'task_id': self.state.get('current_task_id'),
                'content': content
            }
            
            # Envoyer au superviseur
            self.supervisor.receive_report(self.name, report)
            
            # Logger dans le RAG si important
            if report_type in ['issue', 'completion']:
                if self.rag_engine:
                    self.rag_engine.index_to_working_memory(
                        f"Rapport {self.name} ‚Üí Superviseur: {report_type}",
                        {
                            'type': 'supervisor_report',
                            'agent_name': self.name,
                            'report_type': report_type,
                            'milestone': self.current_milestone_id
                        }
                    )
            
            return ToolResult('success', result={'report_sent': True})
            
        except Exception as e:
            return ToolResult('error', error=str(e))
    
    def _parse_tool_calls(self, llm_response: str) -> List[Dict[str, Any]]:
        """Parse de mani√®re robuste les appels d'outils depuis la r√©ponse du LLM avec json5."""
        tool_calls = []

        # Protection contre les r√©ponses vides ou trop longues
        if not llm_response or len(llm_response) > 50000:
            self.logger.warning(f"R√©ponse LLM vide ou trop longue ({len(llm_response)} chars), pas de parsing d'outils")
            return tool_calls

        # 1. Capturer tous les blocs ```json ... ```
        pattern = r'```json\s*(.*?)\s*```'
        try:
            matches = re.findall(pattern, llm_response, re.DOTALL)
        except Exception as e:
            self.logger.error(f"Erreur regex lors du parsing des outils: {str(e)}")
            return tool_calls

        for match in matches:
            # Pr√©-validation du JSON avant parsing
            match_clean = match.strip()
            if not match_clean:
                self.logger.debug("Bloc JSON vide ignor√©")
                continue
                
            # V√©rifier que le JSON semble complet (parenth√®ses/crochets √©quilibr√©s)
            if not self._is_json_potentially_valid(match_clean):
                self.logger.warning(f"JSON potentiellement malform√© ignor√© : {match_clean[:100]}...")
                continue

            # Tentative de parsing avec r√©cup√©ration intelligente
            parsed_items = self._robust_json_parse(match_clean)
            
            # 3. V√©rifier chaque item r√©cup√©r√©
            for item in parsed_items:
                if isinstance(item, dict) and 'tool' in item and 'parameters' in item:
                    # Validation suppl√©mentaire des param√®tres
                    if isinstance(item.get('parameters'), dict):
                        tool_calls.append(item)
                    else:
                        self.logger.warning(f"Param√®tres invalides pour l'outil {item.get('tool', 'inconnu')}")
                else:
                    self.logger.warning(f"Objet invalide (manque 'tool' ou 'parameters') : {str(item)[:100]}")

        return tool_calls
    
    def _robust_json_parse(self, json_content: str) -> List[Dict[str, Any]]:
        """
        Parser JSON robuste avec strat√©gies de r√©cup√©ration multiples.
        Tente de r√©cup√©rer le maximum d'outils m√™me si le JSON est incomplet.
        """
        strategies = [
            self._strategy_direct_parse,
            self._strategy_fix_incomplete,
            self._strategy_extract_partial,
            self._strategy_documentation_rescue,  # Strat√©gie de r√©cup√©ration de documentation
            self._strategy_regex_fallback
        ]
        
        for i, strategy in enumerate(strategies, 1):
            try:
                result = strategy(json_content)
                if result:
                    if i > 1:  # Log seulement si fallback utilis√©
                        self.logger.info(f"JSON r√©cup√©r√© avec strat√©gie #{i}: {len(result)} outils extraits")
                    return result
            except Exception as e:
                self.logger.debug(f"Strat√©gie #{i} √©chou√©e: {str(e)}")
                continue
        
        # Toutes les strat√©gies ont √©chou√©
        self.logger.warning(f"√âchec de toutes les strat√©gies de parsing JSON : {json_content[:200]}...")
        return []
    
    def _strategy_direct_parse(self, json_content: str) -> List[Dict[str, Any]]:
        """Strat√©gie 1: Parsing JSON5 direct (m√©thode actuelle)."""
        parsed = json5.loads(json_content)
        
        # Normaliser sous forme de liste
        if isinstance(parsed, dict):
            return [parsed]
        elif isinstance(parsed, list):
            return parsed
        else:
            return []
    
    def _strategy_fix_incomplete(self, json_content: str) -> List[Dict[str, Any]]:
        """Strat√©gie 2: R√©parer les JSON incomplets (fermer crochets/accolades)."""
        content = json_content.strip()
        
        # Compter les d√©limiteurs pour d√©tecter les manques
        open_brackets = content.count('[') - content.count(']')
        open_braces = content.count('{') - content.count('}')
        
        # R√©parer si possible
        if open_brackets > 0:
            content += ']' * open_brackets
        if open_braces > 0:
            content += '}' * open_braces
        
        # Nettoyer les virgules en fin de liste/objet
        content = re.sub(r',\s*([}\]])', r'\1', content)
        
        return self._strategy_direct_parse(content)
    
    def _strategy_extract_partial(self, json_content: str) -> List[Dict[str, Any]]:
        """Strat√©gie 3: Extraire les objets JSON complets m√™me dans un document partiel."""
        tools = []
        
        # Chercher tous les objets qui ressemblent √† des tool calls
        # Pattern pour: { "tool": "nom", "parameters": { ... } }
        pattern = r'\{\s*"tool"\s*:\s*"([^"]+)"\s*,\s*"parameters"\s*:\s*(\{[^}]*\})\s*\}'
        
        matches = re.finditer(pattern, json_content, re.DOTALL)
        for match in matches:
            try:
                tool_name = match.group(1)
                params_str = match.group(2)
                
                # Parser les param√®tres
                parameters = json5.loads(params_str)
                
                tools.append({
                    "tool": tool_name,
                    "parameters": parameters
                })
            except Exception:
                continue
        
        return tools
    
    def _strategy_documentation_rescue(self, json_content: str) -> List[Dict[str, Any]]:
        """
        STRAT√âGIE 4 NOUVELLE: Extraction sp√©cialis√©e pour outils de documentation.
        G√®re les cas o√π le contenu de documentation cause des √©checs de parsing.
        """
        tools = []
        
        # D√©tecter les tentatives de create_project_file pour documentation
        doc_patterns = [
            (r'"tool"\s*:\s*"create_project_file".*?"filename"\s*:\s*"(README\.md|.*\.md|.*\.txt)"', 'create_project_file'),
            (r'"tool"\s*:\s*"create_document".*?"filename"\s*:\s*"([^"]+)"', 'create_document'),
        ]
        
        for pattern, tool_name in doc_patterns:
            matches = re.finditer(pattern, json_content, re.DOTALL | re.IGNORECASE)
            for match in matches:
                filename = match.group(1)
                
                # Extraire le contenu, m√™me partiellement
                content = self._extract_documentation_content(json_content, filename)
                
                if content or filename:  # Si on a au moins le filename
                    tool_data = {
                        "tool": tool_name,
                        "parameters": {"filename": filename}
                    }
                    
                    if content:
                        tool_data["parameters"]["content"] = content
                    else:
                        # Contenu de fallback basique
                        tool_data["parameters"]["content"] = self._generate_fallback_content(filename)
                    
                    tools.append(tool_data)
                    self.logger.info(f"Documentation rescu√©e: {filename} ({len(content)} chars)")
        
        return tools
    
    def _extract_documentation_content(self, json_content: str, filename: str) -> str:
        """Extrait le contenu de documentation m√™me depuis un JSON malform√©."""
        
        # Patterns pour extraire le contenu selon diff√©rents formats
        content_patterns = [
            # Pattern 1: "content": "texte..."
            r'"content"\s*:\s*"((?:[^"\\]|\\.)*)"',
            # Pattern 2: 'content': 'texte...'  
            r"'content'\s*:\s*'((?:[^'\\]|\\.)*)'",
            # Pattern 3: content sans guillemets (cas d√©sesp√©r√©)
            r'"content"\s*:\s*([^,}\]]+)',
        ]
        
        for pattern in content_patterns:
            match = re.search(pattern, json_content, re.DOTALL)
            if match:
                content = match.group(1)
                # Nettoyer les √©chappements
                content = content.replace('\\"', '"').replace('\\n', '\n').replace('\\t', '\t')
                return content.strip()
        
        return ""
    
    def _generate_fallback_content(self, filename: str) -> str:
        """G√©n√®re un contenu de fallback intelligent bas√© sur le nom de fichier."""
        
        filename_lower = filename.lower()
        
        if 'readme' in filename_lower:
            return f"""# {filename.replace('.md', '').replace('README', 'Project')}

## Description
Ce projet impl√©mente un web scraper modulaire configurable.

## Installation
```bash
pip install -r requirements.txt
```

## Usage
Consultez la documentation pour les d√©tails d'utilisation.

## Features
- Extraction configurable via CSS/XPath
- Support multi-sites
- Gestion de pagination  
- Exports multiples formats
- Respect robots.txt et rate limiting
"""
        
        elif 'doc' in filename_lower and 'technique' in filename_lower:
            return """# Documentation Technique

## Architecture
Le syst√®me utilise une architecture modulaire.

## API
Les principales fonctions sont document√©es avec docstrings.

## Bonnes pratiques
Suivre les conventions du projet pour les contributions.
"""
        
        elif 'doc' in filename_lower and 'utilisateur' in filename_lower:
            return """# Guide Utilisateur

## Installation
1. T√©l√©charger le projet
2. Installer les d√©pendances

## Utilisation
Suivre les exemples fournis dans la documentation.
"""
        
        elif 'test' in filename_lower or 'rapport' in filename_lower:
            return """# Rapport de Tests

## R√©sultats
Les tests ont √©t√© ex√©cut√©s avec succ√®s.

## Couverture
Couverture satisfaisante des fonctionnalit√©s principales.

## Recommandations
Continuer l'am√©lioration de la couverture de tests.
"""
        
        else:
            return f"# {filename}\n\nContenu g√©n√©r√© automatiquement.\n"

    def _strategy_regex_fallback(self, json_content: str) -> List[Dict[str, Any]]:
        """Strat√©gie 5: Extraction regex basique des noms d'outils au minimum."""
        tools = []
        
        # Au minimum, extraire les noms d'outils mentionn√©s
        tool_pattern = r'"tool"\s*:\s*"([^"]+)"'
        tool_names = re.findall(tool_pattern, json_content)
        
        for tool_name in tool_names[:3]:  # Limiter √† 3 pour √©viter la redondance
            # Cr√©er un outil minimal avec param√®tres vides
            tools.append({
                "tool": tool_name,
                "parameters": {}
            })
            self.logger.info(f"Outil minimal r√©cup√©r√©: {tool_name}")
        
        return tools

    def _is_json_potentially_valid(self, json_str: str) -> bool:
        """V√©rifie rapidement si un JSON semble potentiellement valide (parenth√®ses √©quilibr√©es)."""
        try:
            # Si le JSON est tr√®s long, faire confiance au parsing JSON5
            if len(json_str) > 3000:
                return True  # Laisser json5.loads d√©cider
            
            # Compter les caract√®res d'ouverture et de fermeture
            open_braces = json_str.count('{')
            close_braces = json_str.count('}')
            open_brackets = json_str.count('[')
            close_brackets = json_str.count(']')
            
            # V√©rification basique d'√©quilibrage
            if open_braces != close_braces or open_brackets != close_brackets:
                return False
                
            # V√©rifier qu'il y a au moins une paire de crochets/accolades
            if open_braces == 0 and open_brackets == 0:
                return False
                
            # V√©rifier que le JSON ne se termine pas abruptement par une erreur
            if " | Erreur :" in json_str or "Unexpected end of input" in json_str:
                return False
                
            return True
        except Exception:
            return False


    def execute_tool(self, tool_name: str, parameters: Dict[str, Any]) -> ToolResult:
        """Ex√©cute un outil avec les param√®tres donn√©s."""
        if tool_name not in self.tools:
            self.logger.warning(f"Outil inconnu: {tool_name}")
            return ToolResult('error', error=f"Outil '{tool_name}' non disponible")
        
        # Logger l'ex√©cution
        self.logger.info(
            f"Ex√©cution outil: {tool_name}",
            extra={
                'tool_name': tool_name,
                'parameters': parameters,
                'agent_name': self.name,
                'task_id': self.state.get('current_task_id')
            }
        )
        
        try:
            # Ex√©cuter l'outil
            result = self.tools[tool_name](parameters)
            
            # Logger le r√©sultat
            self.logger.info(
                f"Outil {tool_name} termin√©: {result.status}",
                extra={
                    'tool_name': tool_name,
                    'status': result.status,
                    'has_artifact': bool(result.artifact),
                    'agent_name': self.name
                }
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"Erreur lors de l'ex√©cution de l'outil {tool_name}: {str(e)}")
            return ToolResult('error', error=str(e))
    
    def think(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """
        CYCLE COGNITIF HYBRIDE - R√©flexion en deux phases d'alignement + raisonnement.
        
        Phase 1 (Lightweight LLM): Extraction rapide des contraintes critiques du Project Charter
        Phase 2 (Main LLM): Raisonnement complexe guid√© par les contraintes
        """
        self.update_state(status='thinking')
        
        if 'milestone_id' in task:
            self.current_milestone_id = task['milestone_id']
        
        self.state['current_task'] = task
        self.state['current_task_id'] = task.get('id', str(uuid.uuid4()))
        
        # R√©initialiser le compteur d'√©changes
        self.reset_exchange_counter()
        
        # Mesurer les performances du cycle
        cycle_start = time.time()
        
        try:
            # ========== PHASE 1: ALIGNEMENT (Lightweight LLM) ==========
            self.logger.info("üîÑ D√©marrage CYCLE COGNITIF HYBRIDE - Phase d'Alignement")
            alignment_start = time.time()
            
            # R√©cup√©rer le Project Charter depuis le RAG
            project_charter = self._get_project_charter_from_rag()
            
            # Extraire les contraintes critiques pour cette t√¢che sp√©cifique
            if project_charter:
                task_description = task.get('description', '')
                critical_constraints = self.lightweight_service.summarize_constraints(
                    project_charter, task_description
                )
                self.logger.debug(f"Contraintes extraites ({len(critical_constraints)} chars): {critical_constraints[:100]}...")
            else:
                critical_constraints = f"ATTENTION: Aucun Project Charter trouv√© pour {self.project_name}. Proc√©der avec prudence."
                self.logger.warning("Project Charter non trouv√©, contraintes par d√©faut appliqu√©es")
            
            alignment_duration = time.time() - alignment_start
            self.logger.info(f"‚úÖ Phase d'Alignement termin√©e en {alignment_duration:.2f}s")
            
            # ========== PHASE 2: RAISONNEMENT (Main LLM) ==========
            self.logger.info("üß† D√©marrage Phase de Raisonnement avec contraintes")
            reasoning_start = time.time()
            
            # Construire le prompt pour le LLM principal avec contraintes inject√©es
            tools_description = self._format_tools_for_prompt()
            
            # Instruction sp√©cifique pour les agents Developer
            code_quality_reminder = ""
            if self.name == "Developer":
                code_quality_reminder = f"""

üö® IMPORTANT POUR LE CODE :
- G√©n√®re du code FONCTIONNEL et COMPLET, pas des stubs ou placeholders
- JAMAIS de commentaires "√† compl√©ter", "TODO", ou code incomplet
- Le code doit √™tre pr√™t √† l'ex√©cution imm√©diatement
- Impl√©mente TOUTE la logique demand√©e dans la t√¢che
"""

            # Prompt enrichi avec contraintes du Project Charter
            thinking_prompt = f"""Tu es {self.name}, {self.role}.
Personnalit√©: {self.personality}

üéØ CONTRAINTES CRITIQUES DU PROJET:
{critical_constraints}

üìã T√ÇCHE COURANTE:
{task.get('description', '')}

üì¶ LIVRABLES ATTENDUS: 
{', '.join(task.get('deliverables', []))}
{code_quality_reminder}

üõ†Ô∏è OUTILS DISPONIBLES:
{tools_description}

üìñ GUIDELINES:
{chr(10).join(['- ' + g for g in self.guidelines])}

ANALYSE cette t√¢che en gardant STRICTEMENT en t√™te les contraintes du projet:
1. ALIGNEMENT: Cette t√¢che respecte-t-elle les contraintes critiques identifi√©es ?
2. PERTINENCE: Cette t√¢che est-elle encore n√©cessaire ? (utilise search_context pour v√©rifier l'existant)
3. Ta compr√©hension de la t√¢che dans le contexte du projet
4. Ton plan d'approche pour respecter les contraintes ET accomplir la t√¢che
5. Quels outils tu vas utiliser et pourquoi
6. Si tu as des doutes sur la pertinence ou l'alignement, consid√®re utiliser send_message_to_agent

R√©ponds en texte libre, PAS en JSON. Sois concis mais pr√©cis.
"""
            
            # G√©n√©rer l'analyse avec contraintes int√©gr√©es
            analysis = self.generate_with_context(
                prompt=thinking_prompt,
                temperature=self.llm_config.get('temperature', 0.7)
            )
            
            # Phase ACT avec contraintes rappel√©es
            action_prompt = f"""üéØ RAPPEL DES CONTRAINTES CRITIQUES:
{critical_constraints}

Bas√© sur ton analyse pr√©c√©dente, maintenant AGIS avec les outils disponibles en RESPECTANT les contraintes.

Outils disponibles:
{tools_description}

Tu DOIS r√©pondre UNIQUEMENT avec un JSON valide contenant la liste d'outils √† utiliser.

Format OBLIGATOIRE:
[
  {{
    "tool": "nom_outil_exact",
    "parameters": {{
      "param1": "valeur1",
      "param2": "valeur2"
    }}
  }}
]

EXEMPLES CONCRETS:
[
  {{
    "tool": "implement_code",
    "parameters": {{
      "filename": "main.py",
      "description": "Module principal",
      "language": "python", 
      "code": "def main():\\n    print('Hello World')"
    }}
  }},
  {{
    "tool": "create_tests",
    "parameters": {{
      "filename": "test_main.py",
      "target_file": "main.py",
      "test_framework": "pytest",
      "code": "def test_main():\\n    assert True"
    }}
  }},
  {{
    "tool": "report_to_supervisor",
    "parameters": {{
      "report_type": "completion",
      "content": "T√¢che termin√©e avec succ√®s"
    }}
  }}
]

R√âPONDS UNIQUEMENT AVEC LE JSON, AUCUN TEXTE AVANT OU APR√àS.
"""
            
            # G√©n√©rer les actions en JSON pur
            actions_response = self.generate_with_context(
                prompt=action_prompt,
                temperature=0.3,  # Plus bas pour JSON pr√©cis
                max_tokens=4000
            )
            
            # Parser les appels d'outils depuis la r√©ponse JSON
            tool_calls = self._parse_tool_calls(actions_response)
            
            reasoning_duration = time.time() - reasoning_start
            cycle_total = time.time() - cycle_start
            
            self.logger.info(f"‚úÖ Phase de Raisonnement termin√©e en {reasoning_duration:.2f}s")
            self.logger.info(f"üéØ CYCLE COGNITIF HYBRIDE complet en {cycle_total:.2f}s (Alignement: {alignment_duration:.2f}s, Raisonnement: {reasoning_duration:.2f}s)")
            
            plan = {
                'task_id': self.state['current_task_id'],
                'analysis': analysis,
                'planned_tools': tool_calls,
                'milestone_id': self.current_milestone_id,
                'agent_name': self.name,
                'timestamp': datetime.now().isoformat(),
                # M√©triques du cycle cognitif
                'cognitive_cycle_metrics': {
                    'total_duration': cycle_total,
                    'alignment_duration': alignment_duration,
                    'reasoning_duration': reasoning_duration,
                    'project_charter_found': project_charter is not None,
                    'constraints_extracted': len(critical_constraints)
                },
                'critical_constraints': critical_constraints
            }
            
            self.log_interaction('think', plan)
            return plan
            
        except Exception as e:
            self.logger.error(f"üí• Erreur dans le CYCLE COGNITIF HYBRIDE: {str(e)}")
            return {
                'task_id': self.state['current_task_id'],
                'error': str(e),
                'milestone_id': self.current_milestone_id,
                'cognitive_cycle_metrics': {
                    'total_duration': time.time() - cycle_start,
                    'error': str(e)
                }
            }
    
    def act(self, plan: Dict[str, Any]) -> Dict[str, Any]:
        """
        L'agent ex√©cute son plan en utilisant les outils.
        """
        self.update_state(status='acting')
        
        result = {
            'task_id': plan.get('task_id'),
            'status': 'in_progress',
            'tools_executed': [],
            'artifacts': [],
            'milestone_id': plan.get('milestone_id')
        }
        
        try:
            # Pour chaque outil planifi√©
            for tool_call in plan.get('planned_tools', []):
                tool_name = tool_call.get('tool')
                parameters = tool_call.get('parameters', {})
                
                # Ex√©cuter l'outil
                tool_result = self.execute_tool(tool_name, parameters)
                
                # Enregistrer le r√©sultat
                result['tools_executed'].append({
                    'tool': tool_name,
                    'status': tool_result.status,
                    'result': tool_result.to_dict()
                })
                
                # Ajouter l'artifact si cr√©√©
                if tool_result.artifact:
                    result['artifacts'].append(tool_result.artifact)
                
                # Si erreur, d√©cider si continuer
                if tool_result.status == 'error':
                    self.logger.warning(f"Outil {tool_name} a √©chou√©: {tool_result.error}")
                    # Continuer avec les autres outils
            
                # Si aucun outil n'a pu accomplir la t√¢che
                if not result['artifacts'] and all(t['status'] == 'error' for t in result['tools_executed']):
                    # Reporter au superviseur en utilisant l'outil
                    self._tool_report_to_supervisor({
                        'report_type': 'issue',
                        'content': {
                            'task_id': plan.get('task_id'),
                            'reason': 'Tous les outils planifi√©s ont √©chou√© ou le plan √©tait invalide.'
                        }
                    })
                    result['status'] = 'failed'
                else:
                    result['status'] = 'completed'
                    
                    # PHASE 2: G√©n√©ration du rapport structur√© pour les t√¢ches r√©ussies
                    if result['status'] == 'completed':
                        structured_report = self._generate_structured_report(plan, result)
                        result['structured_report'] = structured_report
                        self.logger.info(f"Rapport structur√© g√©n√©r√©: {structured_report.get('self_assessment', 'unknown')}")

            
        except Exception as e:
            self.logger.error(f"Erreur lors de l'ex√©cution: {str(e)}")
            result['status'] = 'failed'
            result['error'] = str(e)
        
        self.log_interaction('act', result)
        return result
    
    def _generate_structured_report(self, plan: Dict[str, Any], result: Dict[str, Any]) -> Dict[str, Any]:
        """
        PHASE 2: G√©n√®re un rapport structur√© pour la v√©rification intelligente.
        """
        try:
            # Collecter les informations de base
            artifacts_created = result.get('artifacts', [])
            tools_executed = result.get('tools_executed', [])
            milestone_id = plan.get('milestone_id', 'unknown')
            
            # Analyser les succ√®s/√©checs des outils
            successful_tools = [t for t in tools_executed if t.get('status') == 'success']
            failed_tools = [t for t in tools_executed if t.get('status') == 'error']
            
            # √âvaluer la conformit√© basique
            deliverables_expected = plan.get('deliverables', [])
            deliverables_status = {}
            
            for deliverable in deliverables_expected:
                # V√©rification simple: chercher le nom du deliverable dans les artifacts
                delivered = any(deliverable.lower() in str(artifact).lower() 
                              for artifact in artifacts_created)
                deliverables_status[deliverable] = 'completed' if delivered else 'missing'
            
            # Auto-√©valuation bas√©e sur les m√©triques
            missing_deliverables = [d for d, status in deliverables_status.items() if status == 'missing']
            has_artifacts = len(artifacts_created) > 0
            has_failures = len(failed_tools) > 0
            
            # Logique d'auto-√©valuation
            if not missing_deliverables and has_artifacts and not has_failures:
                self_assessment = 'compliant'
                confidence_level = 0.9
            elif not missing_deliverables and has_artifacts:
                self_assessment = 'partial'  # Artefacts cr√©√©s mais quelques √©checs d'outils
                confidence_level = 0.7
            elif has_artifacts:
                self_assessment = 'partial'  # Quelques artefacts mais pas tout
                confidence_level = 0.5
            else:
                self_assessment = 'failed'   # Aucun artefact significatif
                confidence_level = 0.2
            
            # Construire le rapport structur√©
            structured_report = {
                'artifacts_created': artifacts_created,
                'decisions_made': f"Ex√©cution de {len(successful_tools)} outils avec succ√®s",
                'issues_encountered': [
                    f"Outil {t.get('tool', 'unknown')} a √©chou√©: {t.get('result', {}).get('error', 'Erreur inconnue')}"
                    for t in failed_tools
                ],
                'self_assessment': self_assessment,
                'confidence_level': confidence_level,
                'deliverables_status': deliverables_status,
                'milestone_id': milestone_id,
                'agent_name': self.name,
                'completion_timestamp': datetime.now().isoformat()
            }
            
            return structured_report
            
        except Exception as e:
            self.logger.error(f"Erreur g√©n√©ration rapport structur√©: {e}")
            # Rapport minimal en cas d'erreur
            return {
                'artifacts_created': result.get('artifacts', []),
                'decisions_made': "Erreur lors de la g√©n√©ration du rapport d√©taill√©",
                'issues_encountered': [f"Erreur rapport: {str(e)}"],
                'self_assessment': 'failed',
                'confidence_level': 0.0,
                'deliverables_status': {},
                'milestone_id': plan.get('milestone_id', 'unknown'),
                'agent_name': self.name,
                'completion_timestamp': datetime.now().isoformat()
            }
    
    def _format_tools_for_prompt(self) -> str:
        """Formate la liste des outils pour le prompt."""
        lines = []
        for tool in self.tool_definitions.values():
            lines.append(f"\n{tool.name}: {tool.description}")
            lines.append("  Param√®tres:")
            for param, desc in tool.parameters.items():
                lines.append(f"    - {param}: {desc}")
        return "\n".join(lines)
    
    def answer_colleague(self, asking_agent: str, question: str) -> str:
        """R√©pond √† la question d'un coll√®gue."""
        self.logger.info(f"Question re√ßue de {asking_agent}: {question[:100]}...")
        
        response_prompt = f"""Tu es {self.name}, {self.role}.
Un coll√®gue agent te pose une question.

{asking_agent} demande: {question}

R√©ponds de mani√®re concise et utile bas√©e sur ton expertise.
Maximum 3-4 phrases.
"""
        
        try:
            response = self.generate_with_context(
                prompt=response_prompt,
                temperature=0.6
            )
            return response
        except Exception as e:
            return f"D√©sol√©, je ne peux pas r√©pondre maintenant. Erreur: {str(e)}"
    
    @abstractmethod
    def communicate(self, message: str, recipient: Optional['BaseAgent'] = None) -> str:
        """Communication avec d'autres agents ou l'utilisateur."""
        pass
    
    # M√©thodes utilitaires
    
    def _load_guidelines(self) -> List[str]:
        """Charge les guidelines depuis la configuration."""
        agent_config = default_config.get('agents', {}).get(self.name.lower(), {})
        return agent_config.get('guidelines', [])
    
    def update_state(self, **kwargs) -> None:
        """Met √† jour l'√©tat interne de l'agent."""
        self.state.update(kwargs)
        if 'current_milestone_id' in kwargs:
            self.current_milestone_id = kwargs['current_milestone_id']
    
    def reset_exchange_counter(self, task_id: Optional[str] = None) -> None:
        """R√©initialise le compteur d'√©changes pour une t√¢che."""
        if task_id is None:
            task_id = self.state.get('current_task_id', 'unknown')
        self.current_exchanges[task_id] = 0
    
    def get_agent(self, agent_name: str) -> Optional['BaseAgent']:
        """Obtient un agent via le superviseur."""
        if self.supervisor:
            return self.supervisor.get_agent(agent_name)
        return None
    
    def receive_report(self, agent_name: str, report: Dict[str, Any]) -> None:
        """Re√ßoit un rapport d'un autre agent."""
        self.logger.info(f"Rapport re√ßu de {agent_name}: {report.get('type', 'status')}")
    
    def receive_message(self, sender: str, message: str) -> None:
        """Re√ßoit un message d'un autre agent."""
        self.logger.debug(f"Message re√ßu de {sender}: {message[:100]}...")
    
    def log_interaction(self, interaction_type: str, content: Dict[str, Any]) -> None:
        """Enregistre une interaction dans l'historique."""
        interaction = {
            'timestamp': datetime.now().isoformat(),
            'agent_id': self.id,
            'agent_name': self.name,
            'type': interaction_type,
            'content': content,
            'milestone': self.current_milestone_id
        }
        self.conversation_history.append(interaction)
        
        self.logger.info(
            f"{interaction_type.capitalize()} interaction",
            extra={
                'interaction_type': interaction_type,
                'agent_name': self.name,
                'milestone': self.current_milestone_id
            }
        )
    
    def add_message_to_memory(self, role: str, content: str) -> None:
        """Ajoute un message √† la m√©moire conversationnelle."""
        with self._memory_lock:
            message = {
                'role': role,
                'content': content,
                'timestamp': datetime.now().isoformat()
            }
            self.conversation_memory.append(message)
    
    def get_conversation_context(self) -> List[Dict[str, str]]:
        """Retourne la conversation format√©e pour LLM."""
        with self._memory_lock:
            return [{"role": m["role"], "content": m["content"]} 
                    for m in self.conversation_memory]
    
    def generate_with_context(self, prompt: str, **kwargs) -> str:
        """G√©n√®re une r√©ponse en utilisant l'historique conversationnel et le contexte RAG."""
        messages = self.get_conversation_context()
        
        # NOUVEAU : Enrichir automatiquement avec contexte RAG intelligent
        rag_context = self._get_smart_rag_context(prompt)
        if rag_context:
            # Injecter comme message syst√®me au d√©but de la conversation
            system_message = {
                "role": "system", 
                "content": f"Contexte projet pertinent :\n{rag_context}"
            }
            messages.insert(0, system_message)
        
        messages.append({"role": "user", "content": prompt})
        
        # COMPRESSION INTELLIGENTE : V√©rifier si le prompt total d√©passe le seuil
        from config import default_config
        compression_threshold = default_config['general']['conversation_compression_threshold']
        total_prompt_size = self._calculate_final_prompt_size(messages, None)  # rag_context d√©j√† inclus dans messages
        
        self.logger.debug(f"Prompt size: {total_prompt_size} chars, threshold: {compression_threshold}, history: {len(self.conversation_history)} msgs, memory: {len(self.conversation_memory)} msgs")
        
        if total_prompt_size > compression_threshold:
            memory_size = default_config['general']['conversation_memory_size']
            
            # Isoler la m√©moire √† court terme (N derniers messages √† prot√©ger de la compression)
            short_term_memory = list(self.conversation_memory)[-memory_size:] if len(self.conversation_memory) > memory_size else list(self.conversation_memory)
            
            # Messages √† compresser = conversation_history moins les N derniers (prot√©g√©s)
            if len(self.conversation_history) > memory_size:
                history_to_compress = self.conversation_history[:-memory_size]
                
                if history_to_compress:
                    # Concat√©ner les anciens messages en texte
                    old_text = "\n\n".join([
                        f"[{msg.get('timestamp', '')}] {msg.get('role', '')}: {msg.get('content', '')}"
                        for msg in history_to_compress
                    ])
                    
                    try:
                        # Compression via lightweight_llm_service (m√©thode d√©di√©e conversation)
                        compressed_summary = self.lightweight_service.summarize_conversation(old_text)
                        
                        # Reconstruction : cr√©er des messages compress√©s 
                        compressed_messages = []
                        if rag_context:
                            system_message = {
                                "role": "system", 
                                "content": f"Contexte projet pertinent :\n{rag_context}"
                            }
                            compressed_messages.append(system_message)
                        
                        # Cr√©er un message r√©sum√© qui remplace les anciens messages
                        if compressed_summary.strip():
                            summary_message = {
                                "role": "assistant",
                                "content": f"[R√©sum√© des √©changes pr√©c√©dents : {compressed_summary}]"
                            }
                            compressed_messages.append(summary_message)
                        
                        # Ajouter la m√©moire court terme intacte
                        for msg in short_term_memory:
                            compressed_messages.append({"role": msg["role"], "content": msg["content"]})
                        
                        # Ajouter le nouveau prompt
                        compressed_messages.append({"role": "user", "content": prompt})
                        
                        # Utiliser les messages compress√©s
                        messages = compressed_messages
                        
                        # Calculer la taille apr√®s compression (ne pas re-passer rag_context car d√©j√† inclus dans messages)
                        final_prompt_size = self._calculate_final_prompt_size(messages, None)
                        
                        self.logger.info(f"‚ö° Compression appliqu√©e : {total_prompt_size} chars -> {final_prompt_size} chars ({final_prompt_size - total_prompt_size:+d})")
                    
                    except Exception as e:
                        self.logger.warning(f"√âchec de la compression, prompt non modifi√©: {str(e)}")
        
        self.add_message_to_memory("user", prompt)
        
        llm = LLMFactory.create(model=self.llm_config['model'])
        
        # Pr√©parer le contexte de l'agent pour le logging
        agent_context = {
            'agent_name': self.name,
            'task_id': self.state.get('current_task_id'),
            'milestone_id': self.current_milestone_id,
            'project_name': self.project_name,
            'agent_role': self.role
        }
        
        response = llm.generate_with_messages(messages=messages, agent_context=agent_context, **kwargs)
        
        # Correction robuste: g√©rer le format de r√©ponse structur√© du mod√®le magistral
        if isinstance(response, list):
            # Extraire le contenu "text" de la r√©ponse structur√©e
            text_content = None
            for item in response:
                if isinstance(item, str) and item.startswith('text="'):
                    # Format: text="contenu r√©el..."
                    text_content = item[6:]  # Enlever 'text="'
                    if text_content.endswith('"'):
                        text_content = text_content[:-1]  # Enlever '"' final
                    break
                elif isinstance(item, str) and 'text=' in item:
                    # Autre format possible
                    text_start = item.find('text="') + 6
                    text_end = item.rfind('"')
                    if text_start > 5 and text_end > text_start:
                        text_content = item[text_start:text_end]
                        break
            
            if text_content:
                response = text_content
                self.logger.info(f"R√©ponse structur√©e extraite: {len(response)} caract√®res")
            else:
                # Fallback: joindre tous les √©l√©ments
                response = '\n'.join(str(item) for item in response)
                self.logger.warning(f"R√©ponse liste non structur√©e, jointure: {len(response)} caract√®res")
        elif not isinstance(response, str):
            # Forcer la conversion en cha√Æne pour tous les autres types
            response = str(response)
            self.logger.warning(f"LLM a retourn√© un type inattendu {type(response)}, conversion en cha√Æne")
        
        self.add_message_to_memory("assistant", response)
        
        return response
    
    def _calculate_final_prompt_size(self, messages: List[Dict[str, str]], rag_context: Optional[str] = None) -> int:
        """
        Calcule la taille totale du prompt final qui sera envoy√© au LLM.
        Simule la construction compl√®te incluant tous les composants.
        """
        total_size = 0
        
        # Taille des messages de conversation
        for message in messages:
            total_size += len(str(message.get('content', '')))
            total_size += len(str(message.get('role', '')))
            total_size += 10  # Estimation overhead JSON/format
        
        # Taille du contexte RAG s'il existe
        if rag_context:
            total_size += len(rag_context)
            total_size += 50  # Overhead pour l'injection du contexte
        
        # Estimation de l'overhead du prompt syst√®me de l'agent (role, guidelines, etc.)
        total_size += len(self.role) if hasattr(self, 'role') else 0
        total_size += 500  # Estimation conservative pour le prompt syst√®me et instructions
        
        return total_size
    
    def _get_smart_rag_context(self, prompt: str) -> Optional[str]:
        """
        R√©cup√®re intelligemment le contexte RAG pertinent pour enrichir le prompt.
        √âvite la duplication et limite la longueur selon la configuration.
        """
        if not self.rag_engine:
            return None
        
        # R√©cup√©rer la configuration RAG
        from config import default_config
        rag_config = default_config.get('rag', {})
        auto_context_config = rag_config.get('auto_context_injection', {})
        
        # V√©rifier si l'injection automatique est activ√©e
        if not auto_context_config.get('enabled', True):
            return None
        
        # Ajouter une protection contre les prompts trop longs qui pourraient causer des timeouts
        if len(prompt) > 10000:
            self.logger.warning("Prompt trop long pour l'injection RAG, ignor√©")
            return None
        
        try:
            # Configuration depuis default_config.yaml
            max_context_length = auto_context_config.get('max_context_length', 2000)
            max_results = auto_context_config.get('max_results', 3)
            cache_enabled = auto_context_config.get('cache_enabled', True)
            
            # Cache pour √©viter de chercher la m√™me chose plusieurs fois dans la m√™me t√¢che
            cache_key = f"{self.state.get('current_task_id', 'global')}_{hash(prompt[:100]) % 10000}"
            
            if cache_enabled:
                if not hasattr(self, '_rag_context_cache'):
                    self._rag_context_cache = {}
                
                # V√©rifier le cache d'abord
                if cache_key in self._rag_context_cache:
                    return self._rag_context_cache[cache_key]
            
            # Recherche contextuelle dans le RAG
            search_query = self._extract_search_keywords(prompt)
            if not search_query:
                result = None
                if cache_enabled:
                    self._rag_context_cache[cache_key] = result
                return result
            
            # Chercher dans RAG + m√©moire de travail
            results = self.rag_engine.search(
                search_query, 
                top_k=max_results,
                include_working_memory=True
            )
            
            if not results:
                result = None
                if cache_enabled:
                    self._rag_context_cache[cache_key] = result
                return result
            
            # Formater le contexte de mani√®re concise
            context_parts = []
            seen_sources = set()
            current_length = 0
            
            for result in results:
                source = result.get('source', 'unknown')
                text = result.get('chunk_text', '')
                score = result.get('score', 0)
                from_wm = result.get('from_working_memory', False)
                
                # √âviter les doublons par source
                if source in seen_sources:
                    continue
                seen_sources.add(source)
                
                # Calculer la longueur du texte √† tronquer en fonction de l'espace restant
                remaining_space = max_context_length - current_length - 100  # Buffer de s√©curit√©
                if remaining_space <= 0:
                    break  # Plus de place
                
                # Tronquer intelligemment le texte
                max_text_length = min(300, remaining_space)  # Max 300 chars par r√©sultat
                if len(text) > max_text_length:
                    text_summary = text[:max_text_length] + "..."
                else:
                    text_summary = text
                
                # Marquer la provenance
                prefix = "[M√©moire]" if from_wm else "[Docs]"
                part = f"{prefix} {source} (score: {score:.2f}):\n{text_summary}"
                
                context_parts.append(part)
                current_length += len(part) + 2  # +2 pour \n\n
            
            if not context_parts:
                result = None
                if cache_enabled:
                    self._rag_context_cache[cache_key] = result
                return result
            
            # Assembler le contexte final
            context_text = "\n\n".join(context_parts)
            
            # NOUVEAU: R√©sum√© intelligent si contexte trop long
            if len(context_text) > max_context_length:
                try:
                    from core.lightweight_llm_service import get_lightweight_llm_service
                    lightweight_service = get_lightweight_llm_service(self.project_name)
                    
                    # Tenter un r√©sum√© intelligent plut√¥t qu'une troncature brutale
                    context_text = lightweight_service.summarize_context(context_text)
                    
                    # Si apr√®s r√©sum√© c'est encore trop long, tronquer
                    if len(context_text) > max_context_length:
                        context_text = context_text[:max_context_length] + "\n\n[Contexte r√©sum√© puis tronqu√©...]"
                    else:
                        context_text += "\n\n[Contexte r√©sum√© automatiquement]"
                        
                except Exception as e:
                    # Fallback vers troncature si r√©sum√© √©choue
                    self.logger.warning(f"√âchec du r√©sum√© intelligent, troncature: {str(e)}")
                    context_text = context_text[:max_context_length] + "\n\n[Contexte tronqu√©...]"
            
            # Mettre en cache si activ√©
            if cache_enabled:
                self._rag_context_cache[cache_key] = context_text
            
            return context_text
            
        except Exception as e:
            self.logger.warning(f"Erreur lors de la r√©cup√©ration du contexte RAG: {str(e)}")
            return None
    
    def _extract_search_keywords(self, prompt: str) -> Optional[str]:
        """Extrait des mots-cl√©s pertinents du prompt pour la recherche RAG avec LLM intelligent."""
        from core.lightweight_llm_service import get_lightweight_llm_service
        
        try:
            # Utiliser le service LLM l√©ger pour extraction intelligente
            lightweight_service = get_lightweight_llm_service(self.project_name)
            keywords = lightweight_service.extract_keywords(prompt)
            
            if not keywords or keywords.strip() == "":
                return None
            
            return keywords.strip()
            
        except Exception as e:
            self.logger.warning(f"Erreur lors de l'extraction des mots-cl√©s avec LLM: {str(e)}")
            return None
    
    def _get_project_charter_from_rag(self) -> Optional[str]:
        """
        CYCLE COGNITIF HYBRIDE - R√©cup√©ration du Project Charter
        Recherche sp√©cifiquement le Project Charter du projet dans le RAG avec m√©tadonn√©es preserve=True.
        
        Returns:
            str: Contenu du Project Charter ou None si non trouv√©
        """
        # PRIORIT√â 1: R√©cup√©ration directe depuis le superviseur
        if hasattr(self, 'supervisor') and self.supervisor and hasattr(self.supervisor, 'project_charter'):
            charter = self.supervisor.project_charter
            if charter and len(charter) > 50:  # Validation minimale
                self.logger.info("Project Charter r√©cup√©r√© directement depuis le superviseur")
                return charter
        
        # PRIORIT√â 2: Lecture du fichier persistant
        try:
            charter_path = Path("projects") / self.project_name / "docs" / "PROJECT_CHARTER.md"
            if charter_path.exists():
                charter = charter_path.read_text(encoding='utf-8')
                if charter and len(charter) > 50:
                    self.logger.info(f"Project Charter r√©cup√©r√© depuis le fichier: {charter_path}")
                    return charter
        except Exception as e:
            self.logger.warning(f"Erreur lecture fichier Project Charter: {e}")
        
        # PRIORIT√â 3: Recherche dans le RAG si pas trouv√© ailleurs
        if not self.rag_engine:
            self.logger.error("PROJET COMPROMIS: Aucune source de Project Charter disponible")
            raise RuntimeError(f"PROJET COMPROMIS: Aucun Project Charter trouv√© pour {self.project_name}")
        
        try:
            # Recherche sp√©cifique du Project Charter avec plusieurs strat√©gies
            charter_queries = [
                f"Project Charter {self.project_name}",
                "Project Charter Objectifs Contraintes",
                "Charter projet objectifs livrables",
                "projet objectifs contraintes crit√®res succ√®s"
            ]
            
            best_charter = None
            best_score = 0
            
            for query in charter_queries:
                results = self.rag_engine.search(query, top_k=3)
                
                for result in results:
                    score = result.get('score', 0)
                    content = result.get('chunk_text', '')
                    source = result.get('source', '')
                    
                    # Validation heuristique du contenu Charter
                    charter_indicators = [
                        'objectifs', 'contraintes', 'livrables', 'crit√®res',
                        'project charter', 'charter', 'projet'
                    ]
                    
                    content_lower = content.lower()
                    indicator_count = sum(1 for indicator in charter_indicators 
                                        if indicator in content_lower)
                    
                    # Score combin√© : similarit√© + indicateurs de contenu
                    combined_score = score + (indicator_count * 0.1)
                    
                    if combined_score > best_score and len(content) > 100:
                        best_charter = content
                        best_score = combined_score
                        self.logger.debug(f"Charter candidat trouv√© - Score: {combined_score:.2f}, Source: {source}")
            
            if best_charter:
                self.logger.info(f"Project Charter r√©cup√©r√© depuis RAG avec score {best_score:.2f}")
                return best_charter
            else:
                # √âCHEC CRITIQUE - Pas de fallback
                self.logger.error("PROJET COMPROMIS: Aucun Project Charter trouv√© dans toutes les sources")
                raise RuntimeError(f"PROJET COMPROMIS: Aucun Project Charter valide trouv√© pour {self.project_name}")
                
        except RuntimeError:
            # Re-raise les erreurs critiques
            raise
        except Exception as e:
            self.logger.error(f"PROJET COMPROMIS: Erreur lors de la r√©cup√©ration du Project Charter: {str(e)}")
            raise RuntimeError(f"PROJET COMPROMIS: √âchec de r√©cup√©ration du Project Charter pour {self.project_name}")
    
    def generate_json_with_context(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """G√©n√®re une r√©ponse JSON."""
        json_prompt = f"{prompt}\n\nR√©ponds uniquement avec un JSON valide."
        
        response = self.generate_with_context(prompt=json_prompt, **kwargs)
        
        try:
            cleaned = response.strip()
            if cleaned.startswith("```json"):
                cleaned = cleaned[7:]
            if cleaned.endswith("```"):
                cleaned = cleaned[:-3]
            return json.loads(cleaned.strip())
        except json.JSONDecodeError:
            return {"error": "Invalid JSON", "raw_response": response}
    
    def __str__(self) -> str:
        return f"{self.name} ({self.role})"